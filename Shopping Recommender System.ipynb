{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838dc47-3057-4d24-9cb1-8943360f0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Data Collection and Preprocessing\n",
    "#The first step is to load and preprocess the data. \n",
    "#We collect product data, customer profiles, purchase history, and browsing behavior from the e-commerce platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047868db-b8ed-4d30-ab28-93332601da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  cust_id  tran_date  prod_subcat_code  prod_cat_code  Qty  \\\n",
      "0     80712190438   270351 2014-02-28                 1              1    5   \n",
      "1     80712190438   270351 2014-02-28                 1              1    5   \n",
      "2     80712190438   270351 2014-02-28                 1              1    5   \n",
      "3     80712190438   270351 2014-02-20                 1              1    5   \n",
      "4     80712190438   270351 2014-02-20                 1              1    5   \n",
      "\n",
      "   Rate    Tax  total_amt Store_type  customer_Id        DOB Gender  \\\n",
      "0  -772  405.3     4265.3     e-Shop       270351 1981-09-26      M   \n",
      "1  -772  405.3     4265.3     e-Shop       270351 1981-09-26      M   \n",
      "2  -772  405.3     4265.3     e-Shop       270351 1981-09-26      M   \n",
      "3   772  405.3     4265.3     e-Shop       270351 1981-09-26      M   \n",
      "4   772  405.3     4265.3     e-Shop       270351 1981-09-26      M   \n",
      "\n",
      "   city_code  prod_cat  prod_sub_cat_code prod_subcat  \n",
      "0        5.0  Clothing                  4        Mens  \n",
      "1        5.0  Clothing                  1       Women  \n",
      "2        5.0  Clothing                  3        Kids  \n",
      "3        5.0  Clothing                  4        Mens  \n",
      "4        5.0  Clothing                  1       Women  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# File paths\n",
    "customer_path = 'D:/DATA ANALYSIS/Recommnder/customer.csv'\n",
    "transactions_path = 'D:/DATA ANALYSIS/Recommnder/transactions.csv'\n",
    "prod_cat_info_path = 'D:/DATA ANALYSIS/Recommnder/prod_cat_info.csv'\n",
    "\n",
    "# Load datasets\n",
    "customers = pd.read_csv(customer_path)\n",
    "transactions = pd.read_csv(transactions_path)\n",
    "prod_cat_info = pd.read_csv(prod_cat_info_path)\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "customers['DOB'] = pd.to_datetime(customers['DOB'], format='%d-%m-%Y')\n",
    "transactions['tran_date'] = pd.to_datetime(transactions['tran_date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Handle negative values in 'Qty' and 'total_amt' (treat as returns)\n",
    "transactions['Qty'] = transactions['Qty'].abs()\n",
    "transactions['total_amt'] = transactions['total_amt'].abs()\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = transactions.merge(customers, left_on='cust_id', right_on='customer_Id')\n",
    "merged_df = merged_df.merge(prod_cat_info, on='prod_cat_code')\n",
    "\n",
    "# Display the merged data\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e746d49-6ddb-474d-9ce2-0a065818760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-surprise\n",
      "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.4)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (pyproject.toml): started\n",
      "  Building wheel for scikit-surprise (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-win_amd64.whl size=1297495 sha256=884feb53b9a89eb45415a340afd2130b694c989ee3758f767057c5818308fe64\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\2a\\8f\\6e\\7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "Successfully installed scikit-surprise-1.1.4\n"
     ]
    }
   ],
   "source": [
    "# NOTE- Microsoft Visual C++ Build Tools are required to install the scikit-surprise library on Windows\n",
    "pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98145a-e75c-4a53-91f7-722989a4e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step. 2 2. Recommendation Techniques\n",
    "#Collaborative Filtering (SVD Model)\n",
    "\n",
    "#We implement a Collaborative Filtering technique using Singular Value Decomposition (SVD). \n",
    "#This method helps predict missing ratings based on the patterns of past behavior between users and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2bd87c-8d1c-4a5b-acaa-d275c7bc9e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    6018.77776021.86785982.02905986.29845992.64146000.322816.7040 \n",
      "MAE (testset)     5689.12035692.74085646.86675652.68275660.58945668.400018.9395 \n",
      "Fit time          1.11    1.00    0.97    0.98    1.01    1.01    0.05    \n",
      "Test time         0.27    0.14    0.14    0.26    0.16    0.19    0.06    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([6018.77765649, 6021.86775691, 5982.0290299 , 5986.29835362,\n",
       "        5992.64141421]),\n",
       " 'test_mae': array([5689.12034896, 5692.74076942, 5646.8666602 , 5652.68269992,\n",
       "        5660.58936499]),\n",
       " 'fit_time': (1.1141057014465332,\n",
       "  1.0000813007354736,\n",
       "  0.9740769863128662,\n",
       "  0.9750781059265137,\n",
       "  1.0100939273834229),\n",
       " 'test_time': (0.2682201862335205,\n",
       "  0.14301013946533203,\n",
       "  0.13901090621948242,\n",
       "  0.2630188465118408,\n",
       "  0.15599846839904785)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "\n",
    "# Prepare data for collaborative filtering\n",
    "reader = Reader(rating_scale=(0, merged_df['total_amt'].max()))\n",
    "data = Dataset.load_from_df(merged_df[['cust_id', 'prod_subcat_code', 'total_amt']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train SVD model\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "cross_validate(model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f90d52-1069-4954-82c8-72858b39650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Based Filtering (TF-IDF and Cosine Similarity)\n",
    "#We also implement Content-Based Filtering using TF-IDF vectorization to convert product descriptions into numerical form.\n",
    "#We then calculate the cosine similarity to recommend similar products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d2fb8f-4d66-44e7-81a5-c95bf69a2517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prod_cat_code  prod_cat  prod_sub_cat_code prod_subcat     description\n",
      "1               1  Clothing                  1       Women  Clothing Women\n",
      "3               2  Footwear                  1        Mens   Footwear Mens\n",
      "2               1  Clothing                  3        Kids   Clothing Kids\n",
      "11              4      Bags                  1        Mens       Bags Mens\n",
      "4               2  Footwear                  3       Women  Footwear Women\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create product descriptions by combining category and subcategory\n",
    "prod_cat_info['description'] = prod_cat_info['prod_cat'] + ' ' + prod_cat_info['prod_subcat']\n",
    "\n",
    "# Vectorize the product descriptions\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(prod_cat_info['description'])\n",
    "\n",
    "# Compute cosine similarity between products\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to recommend products based on a given product\n",
    "def recommend_products(product_id, num_recommendations=5):\n",
    "    idx = prod_cat_info.index[prod_cat_info['prod_cat_code'] == product_id][0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:num_recommendations + 1]\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    return prod_cat_info.iloc[product_indices]\n",
    "\n",
    "# Example: Recommend products similar to product with ID 1\n",
    "print(recommend_products(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6668d28-a3f4-4ec8-ae34-4554843bf1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3. Model Evaluation\n",
    "\n",
    "#Finally, we evaluate the performance of the recommender system using various evaluation metrics \n",
    "#such as precision, recall, F1-score, and mean average precision (MAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c8afb5-2d24-452d-b0b0-93c97b631177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Mean Average Precision: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions from the collaborative filtering model\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Extract the actual and predicted ratings from the predictions\n",
    "y_true = np.array([pred.r_ui for pred in predictions])  # Actual ratings from the test set\n",
    "y_pred = np.array([pred.est for pred in predictions])  # Predicted ratings\n",
    "\n",
    "# Convert ratings to binary values (1 if the predicted rating is above a threshold, otherwise 0)\n",
    "threshold = 0.5  # You can adjust this threshold as needed based on your dataset\n",
    "y_true_binary = (y_true > threshold).astype(int)\n",
    "y_pred_binary = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Calculate Precision, Recall, F1-score, and Mean Average Precision (MAP)\n",
    "precision = precision_score(y_true_binary, y_pred_binary)\n",
    "recall = recall_score(y_true_binary, y_pred_binary)\n",
    "f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "map_score = average_precision_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "print(f'Mean Average Precision: {map_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7d9ac-70fe-4a3b-b35b-dd7c4a8569e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
